### Library/Packages Installation and Environment Setup

# Installing necessary packages
!pip install biopython
!pip install transformers
!pip install tensorflow==2.18.0

# Importing required libraries
import os
import numpy as np
import pandas as pd
from Bio import SeqIO
from Bio.Seq import Seq
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

print("Environment setup complete!")
print("TensorFlow version:", tf.__version__)


### Data Downloading and Preprocessing

!mkdir -p /content/sample_data/Pharm.D_Project/data/fastq
!mkdir -p /content/sample_data/Pharm.D_Project/data/reports
!mkdir -p /content/sample_data/Pharm.D_Project/data/trimmed_fastq

df = pd.read_csv("/content/sample_data/Pharm.D_Project/data/SRRs.csv")
srr_list = df.SRR_ID.to_list()

# Update package lists
!apt-get update -qq

# Install sra‑toolkit
!apt-get install -yy sra-toolkit

!apt-get update -qq
!apt-get install -y prodigal
!apt-get install -y cd-hit
!apt-get install -y seqtk
!apt-get install -yy fastp

# Define base paths as before
base_path = "/content/sample_data/Pharm.D_Project/data/"
fastq_path = os.path.join(base_path, "fastq")
trimmed_path = os.path.join(base_path, "trimmed_fastq")
fasta_path = os.path.join(base_path, "fasta")
orfs_path = os.path.join(base_path, "orfs")
reports_path = os.path.join(base_path, "reports")

# Make sure all directories exist
os.makedirs(fastq_path, exist_ok=True)
os.makedirs(trimmed_path, exist_ok=True)
os.makedirs(fasta_path, exist_ok=True)
os.makedirs(orfs_path, exist_ok=True)
os.makedirs(reports_path, exist_ok=True)

# Initialize lists to track success and failure
successful_srr = []
failed_srr = []

# Loop through each SRR accession number
for srr in srr_list:
    print(f"[*] Processing SRR: {srr}")

    try:
        # Step 1: Download and compress the FASTQ files
        !fasterq-dump --split-files {srr} -O {fastq_path}
        !gzip {fastq_path}/{srr}_*.fastq

        # Step 2: Run fastp for quality control
        !fastp \
          -i {fastq_path}/{srr}_1.fastq.gz \
          -I {fastq_path}/{srr}_2.fastq.gz \
          -o {trimmed_path}/{srr}_1.trimmed.fastq.gz \
          -O {trimmed_path}/{srr}_2.trimmed.fastq.gz \
          --detect_adapter_for_pe \
          --qualified_quality_phred 20 \
          --length_required 50 \
          --thread 4 \
          --html {reports_path}/fastp_{srr}_report.html \
          --json {reports_path}/fastp_{srr}_report.json

        # Step 3a: Convert trimmed FASTQ to FASTA
        !seqtk seq -a {trimmed_path}/{srr}_1.trimmed.fastq.gz > {fasta_path}/{srr}_1.trimmed.fasta
        !seqtk seq -a {trimmed_path}/{srr}_2.trimmed.fastq.gz > {fasta_path}/{srr}_2.trimmed.fasta

        # Step 3b: Concatenate FASTA files
        input_fasta = f"{fasta_path}/{srr}.trimmed.fasta"
        !cat {fasta_path}/{srr}_1.trimmed.fasta {fasta_path}/{srr}_2.trimmed.fasta > {input_fasta}


        # Steps 4 & 5: Find Open Reading Frames (ORFs) and generate GFF for traceability
        # I no longer add the SRR to the headers. Instead, I use the original FASTA and
        # the GFF file generated by Prodigal to link the ORF IDs to their source sequence IDs.
        !prodigal \
          -i {input_fasta} \
          -o {orfs_path}/{srr}.prodigal.gff \
          -a {orfs_path}/{srr}.orfs.faa \
          -p meta

        # If all steps succeed, add to the successful list
        successful_srr.append(srr)
        print(f"[✓] Successfully processed SRR: {srr}\n")

    except Exception as e:
        # If any step fails, add to the failed list
        failed_srr.append(srr)
        print(f"[✗] FAILED to process SRR: {srr}. Error: {e}\n")

print("\n\n--- Processing Complete ---")
print(f"Total processed: {len(srr_list)}")
print(f"Successful SRRs: {len(successful_srr)} / {len(srr_list)}")
print(f"Failed SRRs: {len(failed_srr)} / {len(srr_list)}")
print("\nFailed SRR list:")
print(failed_srr)


import os
from Bio import SeqIO
import pandas as pd
import glob

# Define paths
orfs_path = "/content/sample_data/Pharm.D_Project/data/orfs"
cdhit_output = os.path.join(orfs_path, "cdhit_clustered.faa")
output_filtered_fasta = os.path.join(orfs_path, "family_filtered.orfs.faa")
clstr_file = f"{cdhit_output}.clstr"
gff_files = glob.glob(os.path.join(orfs_path, "*.gff"))
combined_faa = os.path.join(orfs_path, "combined.orfs.faa")

# === Step 1: Combine All ORFs for Clustering ===
print("\n[*] Combining all ORF files for clustering...")
with open(combined_faa, "w") as outfile:
    for filename in glob.glob(os.path.join(orfs_path, "*.faa")):
        with open(filename, "r") as infile:
            outfile.write(infile.read())
print(f"[✓] Combined all ORFs into {combined_faa}")

# === Step 2: Run CD-HIT for Clustering ===
print("[*] Running CD-HIT to cluster sequences...")
!cd-hit -i {combined_faa} -o {cdhit_output} -c 0.9 -n 5 -M 0 -T 0
print("[✓] Clustering complete. .clstr file created.")

# === Step 3: Filter and Map to Source for Final Output ===
# Part 3a: Map ORF IDs to original sequence IDs using GFF files
print("\n[*] Creating a map of ORF IDs to original sequences...")
orf_to_source = {}
try:
    for gff_file in gff_files:
        gff_data = pd.read_csv(gff_file, sep='\t', comment='#', header=None,
                               names=['seqname', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute'])
        for _, row in gff_data.iterrows():
            attributes = dict(item.split('=') for item in row['attribute'].split(';'))
            orf_id = attributes['ID']
            orf_to_source[orf_id] = row['seqname']
    print(f"[✓] Mapped {len(orf_to_source)} ORFs to source sequences.")
except Exception as e:
    print(f"[✗] Error reading GFF files: {e}. Cannot map to source sequences.")
    orf_to_source = None

# Part 3b: Parse the .clstr file to find valid cluster IDs
valid_orf_ids = set()
current_cluster_members = []
min_members = 6
print("[*] Parsing .clstr file for valid cluster members...")
with open(clstr_file, 'r') as clstr_handle:
    for line in clstr_handle:
        if line.startswith('>Cluster'):
            if len(current_cluster_members) >= min_members:
                valid_orf_ids.update(current_cluster_members)
            current_cluster_members = []
        else:
            try:
                match = line.split('>')[1].split('...')[0].strip()
                if match.endswith('*'):
                    match = match[:-1]
                current_cluster_members.append(match)
            except IndexError:
                continue
    if len(current_cluster_members) >= min_members:
        valid_orf_ids.update(current_cluster_members)
print(f"[✓] Found {len(valid_orf_ids)} ORFs in clusters with >= {min_members} members.")

# Part 3c: Filter the FASTA file and map to source
filtered_records = []
final_mapping = {}
print("[*] Filtering FASTA file and creating final mapping...")
for record in SeqIO.parse(combined_faa, 'fasta'):
    if record.id in valid_orf_ids:
        filtered_records.append(record)
        if orf_to_source:
            final_mapping[record.id] = orf_to_source.get(record.id, "Unknown_Source")

# Part 3d: Save the filtered sequences and the mapping
SeqIO.write(filtered_records, output_filtered_fasta, 'fasta')

print("\n[✓] Filtering complete.")
print(f"Sequences saved to new file: {len(filtered_records)}")
print(f"New filtered file saved to: {output_filtered_fasta}")

print("\n[✓] Final Mapping of ORF IDs to Original Sequences (Example):")
for orf_id, source_id in list(final_mapping.items())[:5]:
    print(f"  ORF ID: {orf_id} -> Source Sequence: {source_id}")


import os
from Bio import SeqIO
import pandas as pd
import glob

# Define paths (ensure these paths are correct for your environment)
orfs_path = "/content/sample_data/Pharm.D_Project/data/orfs"
cdhit_output = os.path.join(orfs_path, "cdhit_clustered.faa")
output_filtered_fasta = os.path.join(orfs_path, "family_filtered.orfs.faa")
clstr_file = f"{cdhit_output}.clstr"
gff_files = glob.glob(os.path.join(orfs_path, "*.gff"))
combined_faa = os.path.join(orfs_path, "combined.orfs.faa")

# === Step 1: Filter and Map to Source for Final Output ===
# Part 1a: Map ORF IDs to original sequence IDs using GFF files
print("\n[*] Creating a map of ORF IDs to original sequences...")
orf_to_source = {}
try:
    for gff_file in gff_files:
        gff_data = pd.read_csv(
            gff_file,
            sep='\t',
            comment='#',
            header=None,
            names=['seqname', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute'],
            # Corrected line: force 'attribute' column to be a string
            dtype={'attribute': str}
        )
        for _, row in gff_data.iterrows():
            if pd.isna(row['attribute']):
                continue
            attributes = dict(item.split('=') for item in row['attribute'].split(';'))
            orf_id = attributes['ID']
            orf_to_source[orf_id] = row['seqname']
    print(f"[✓] Mapped {len(orf_to_source)} ORFs to source sequences.")
except Exception as e:
    print(f"[✗] Error reading GFF files: {e}. Cannot map to source sequences.")
    orf_to_source = None

# Part 1b: Parse the .clstr file to find valid cluster IDs
valid_orf_ids = set()
current_cluster_members = []
min_members = 6
print("[*] Parsing .clstr file for valid cluster members...")
with open(clstr_file, 'r') as clstr_handle:
    for line in clstr_handle:
        if line.startswith('>Cluster'):
            if len(current_cluster_members) >= min_members:
                valid_orf_ids.update(current_cluster_members)
            current_cluster_members = []
        else:
            try:
                match = line.split('>')[1].split('...')[0].strip()
                if match.endswith('*'):
                    match = match[:-1]
                current_cluster_members.append(match)
            except IndexError:
                continue
    if len(current_cluster_members) >= min_members:
        valid_orf_ids.update(current_cluster_members)
print(f"[✓] Found {len(valid_orf_ids)} ORFs in clusters with >= {min_members} members.")

# Part 1c: Filter the FASTA file and map to source
filtered_records = []
final_mapping = {}
print("[*] Filtering FASTA file and creating final mapping...")
for record in SeqIO.parse(combined_faa, 'fasta'):
    if record.id in valid_orf_ids:
        filtered_records.append(record)
        if orf_to_source:
            final_mapping[record.id] = orf_to_source.get(record.id, "Unknown_Source")

# Part 1d: Save the filtered sequences and the mapping
SeqIO.write(filtered_records, output_filtered_fasta, 'fasta')

print("\n[✓] Filtering complete.")
print(f"Sequences saved to new file: {len(filtered_records)}")
print(f"New filtered file saved to: {output_filtered_fasta}")

print("\n[✓] Final Mapping of ORF IDs to Original Sequences (Example):")
# This will now display correct mappings if the code runs successfully
for orf_id, source_id in list(final_mapping.items())[:5]:
    print(f"  ORF ID: {orf_id} -> Source Sequence: {source_id}")


# Define file paths (if not already defined)
orfs_path = "/content/sample_data/Pharm.D_Project/data/orfs"
filtered_fasta = os.path.join(orfs_path, "family_filtered.orfs.faa")

# --- Part 1: Check the final filtered FASTA file ---
print("[*] Checking the first 5 sequences in the filtered FASTA file...")
try:
    count = 0
    with open(filtered_fasta, "r") as handle:
        for record in SeqIO.parse(handle, "fasta"):
            print(f"> {record.id}")
            print(str(record.seq[:60])) # Print the first 60 characters
            print("...")
            count += 1
            if count >= 5:
                break
except FileNotFoundError:
    print(f"[✗] Error: The file {filtered_fasta} was not found.")

# --- Part 2: Check the final mapping dictionary ---
# This assumes the 'final_mapping' dictionary is still in memory from the last run.
print("\n[*] Checking the first 5 entries in the final mapping dictionary...")
if 'final_mapping' in locals() or 'final_mapping' in globals():
    count = 0
    for orf_id, source_id in final_mapping.items():
        print(f"ORF ID: {orf_id} -> Source ID: {source_id}")
        count += 1
        if count >= 5:
            break
else:
    print("[✗] Error: The 'final_mapping' dictionary was not found in memory.")
    print("If you started a new session, you'll need to rerun the previous script to create it, or use `pickle` to load a saved version.")


# Define paths to your files
orfs_path = "/content/sample_data/Pharm.D_Project/data/orfs"
combined_faa = os.path.join(orfs_path, "combined.orfs.faa")
cdhit_clustered_faa = os.path.join(orfs_path, "cdhit_clustered.faa")
family_filtered_faa = os.path.join(orfs_path, "family_filtered.orfs.faa")

# Function to count sequences in a file
def count_sequences(file_path):
    if not os.path.exists(file_path):
        return "File not found."
    try:
        count = 0
        for _ in SeqIO.parse(file_path, "fasta"):
            count += 1
        return count
    except Exception as e:
        return f"Error reading file: {e}"

# Count and print sequences for each file
print(f"Combined ORFs file: {count_sequences(combined_faa)} sequences")
print(f"CD-HIT clustered file: {count_sequences(cdhit_clustered_faa)} sequences")
print(f"Family-filtered file: {count_sequences(family_filtered_faa)} sequences")


### Model Building 
# Define the source and destination paths
source_path = "/content/sample_data/Pharm.D_Project/data/orfs/family_filtered.orfs.faa"
destination_path = "/content/drive/MyDrive/Pharm.D_Project/data"

# Make sure the destination folder exists
os.makedirs(os.path.dirname(destination_path), exist_ok=True)

# Copy the file
!cp "{source_path}" "{destination_path}"
print(f"[✓] File successfully saved to Google Drive: {destination_path}")


!mkdir -p /content/sample_data/Pharm.D_Project/data/AMPs/
!mkdir -p /content/sample_data/Pharm.D_Project/data/non_AMPs/


from pathlib import Path

# Define the folder paths
amps_folder = Path("/content/sample_data/Pharm.D_Project/data/AMPs")
output_amps_file = amps_folder / "full_AMPs.fasta"

# Define all AMP source files
amp_files = [
    amps_folder / "bacteriaAMPs_APD2024.fasta",
    amps_folder / "humanAMPs_APD2024.fasta",
    amps_folder / "naturalAMPs_APD2024a.fasta"
]

# Combine and deduplicate
seen_seqs = set()
combined_records = []

for file in amp_files:
    if file.exists():
        for record in SeqIO.parse(str(file), "fasta"):
            seq_str = str(record.seq)
            if seq_str not in seen_seqs:
                seen_seqs.add(seq_str)
                combined_records.append(record)

# Save combined file
SeqIO.write(combined_records, str(output_amps_file), "fasta")

# Return basic info
len(combined_records), output_amps_file.name

# from Bio import SeqIO
import matplotlib.pyplot as plt

lengths = [len(rec.seq) for rec in SeqIO.parse("/content/sample_data/Pharm.D_Project/data/AMPs/full_AMPs.fasta", "fasta")]
plt.hist(lengths, bins=20)
plt.xlabel("Sequence length")
plt.ylabel("Frequency")
plt.title("AMP length distribution")
plt.show()

import requests
import time

# === UniProt REST API endpoint ===
base_url = 'https://rest.uniprot.org/uniprotkb/search'

# === Refined Query (Exclude known AMP-related terms) ===
query = (
    'length:[10 TO 100] AND reviewed:true AND '
    'NOT (keyword:"Antimicrobial" OR keyword:"Antibiotic" OR '
    'keyword:"Defensin" OR keyword:"Bacteriocin")'
)

# === Parameters ===
batch_size = 500
target_total = 3307
output_file = '/content/sample_data/Pharm.D_Project/data/non_AMPs/full_non_AMPs.fasta'

# === Set initial URL ===
params = {
    'query': query,
    'format': 'fasta',
    'size': batch_size
}

fetched = 0
next_url = base_url
headers = {"User-Agent": "Mozilla/5.0"}

# === Create or empty the output file first ===
with open(output_file, 'w') as out_f:
    pass

print("[*] Starting download...")

while fetched < target_total:
    response = requests.get(next_url, params=params, headers=headers)

    if response.status_code == 200:
        lines = response.text.strip().split("\n")

        # Append to file
        with open(output_file, 'a') as out_f:
            out_f.write(response.text)

        # Update fetched count (number of '>' lines = number of sequences)
        new_seqs = sum(1 for line in lines if line.startswith(">"))
        fetched += new_seqs
        print(f"[✓] Fetched {new_seqs} sequences (Total: {fetched})")

        # Get the next page URL from response headers
        try:
            next_link = response.links.get("next", {}).get("url")
            if not next_link:
                print("[!] No more pages available.")
                break
            next_url = next_link
            params = None  # cursor URL already has query
            time.sleep(1)  # be kind to UniProt servers
        except Exception as e:
            print(f"[✗] Failed to get next page: {e}")
            break
    else:
        print(f"[✗] Error {response.status_code}: {response.text}")
        break

print(f"[✓] Done! Total non-AMPs downloaded: {fetched}")


import random

input_file = '/content/sample_data/Pharm.D_Project/data/non_AMPs/full_non_AMPs.fasta'
output_file = '/content/sample_data/Pharm.D_Project/data/non_AMPs/balanced_non_AMPs.fasta'

# Read all non-AMPs
all_non_amps = list(SeqIO.parse(input_file, 'fasta'))

# Randomly sample 3307
random.seed(42)
selected = random.sample(all_non_amps, 3307)

# Write to new file
SeqIO.write(selected, output_file, 'fasta')

print(f"[✓] Sampled 3307 non-AMPs saved to: {output_file}")

lengths = [len(rec.seq) for rec in SeqIO.parse("/content/sample_data/Pharm.D_Project/data/non_AMPs/balanced_non_AMPs.fasta", "fasta")]
plt.hist(lengths, bins=20)
plt.xlabel("Sequence length")
plt.ylabel("Frequency")
plt.title("non_AMP length distribution")
plt.show()


records = list(SeqIO.parse('/content/sample_data/Pharm.D_Project/data/non_AMPs/balanced_non_AMPs.fasta', 'fasta'))
print(f"Total non-AMP sequences: {len(records)}")

from Bio import SeqIO
from Bio.SeqRecord import SeqRecord

# === File paths ===
amp_path = "/content/sample_data/Pharm.D_Project/data/AMPs/full_AMPs.fasta"
non_amp_path = "/content/sample_data/Pharm.D_Project/data/non_AMPs/balanced_non_AMPs.fasta"
output_path = "/content/sample_data/Pharm.D_Project/data/merged_balanced_dataset.fasta"

# === Load sequences ===
amp_records = list(SeqIO.parse(amp_path, "fasta"))[:3307]
non_amp_records = list(SeqIO.parse(non_amp_path, "fasta"))[:3307]

# === Label AMPs with class 1 ===
labeled_amp_records = []
for record in amp_records:
    record.id = f"{record.id}|label:1"
    record.description = ""
    labeled_amp_records.append(record)

# === Label non-AMPs with class 0 ===
labeled_non_amp_records = []
for record in non_amp_records:
    record.id = f"{record.id}|label:0"
    record.description = ""
    labeled_non_amp_records.append(record)

# === Combine and save ===
all_records = labeled_amp_records + labeled_non_amp_records
SeqIO.write(all_records, output_path, "fasta")

print(f"[✓] Merged and saved {len(all_records)} sequences to {output_path}")


from Bio import SeqIO
import numpy as np

# Load merged sequences
fasta_file = "/content/sample_data/Pharm.D_Project/data/merged_balanced_dataset.fasta"
sequences = list(SeqIO.parse(fasta_file, "fasta"))

print("Total sequences:", len(sequences))  # Should be exactly 6614

# Create labels: 1 for AMP, 0 for non-AMP
labels = [1]*3307 + [0]*3307

# Confirm label match
assert len(sequences) == len(labels), "Mismatch between sequences and labels"

# Double-check distribution
import numpy as np
print("Label counts:", np.bincount(labels))


# Spacing the amino acids
def space_sequence(seq):
    return ' '.join(str(seq).replace(" ", ""))  # remove any existing spaces and re-space

# Apply spacing
spaced_sequences = [space_sequence(record.seq) for record in sequences]

# Double-check an example
print(spaced_sequences[1])  # preview first spaced sequence

# Confirm the label count
print("Label counts:", np.bincount(labels))


filtered_sequences = []
filtered_labels = []

for seq, label in zip(spaced_sequences, labels):
    if len(seq.replace(" ", "")) >= 9:  # keep if at least 9 amino acids
        filtered_sequences.append(seq)
        filtered_labels.append(label)

# Convert labels to np.array
filtered_labels = np.array(filtered_labels)

print("After filtering:")
print("Total sequences:", len(filtered_sequences))
print("Label counts:", np.bincount(filtered_labels))
print("Example:", filtered_sequences[0])


from transformers import AutoTokenizer
import tensorflow as tf
from sklearn.model_selection import train_test_split
import numpy as np

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("Rostlab/prot_bert_bfd", do_lower_case=False)

# Tokenize all sequences
encodings = tokenizer(
    filtered_sequences,
    padding='max_length',
    truncation=True,
    max_length=190,  # based on your earlier decision
    return_tensors='tf'
)


from sklearn.model_selection import train_test_split

# The filtered and space-separated sequences and labels
sequences = filtered_sequences
labels = filtered_labels

# Step 1: Train and temp (val + test)
X_train, X_temp, y_train, y_temp = train_test_split(
    sequences, labels, test_size=0.30, stratify=labels, random_state=42
)

# Step 2: Validation and Test (split 30% into two 15%s)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42
)

# Confirm splits
print(f"Train set: {len(X_train)}")
print(f"Validation set: {len(X_val)}")
print(f"Test set: {len(X_test)}")

from transformers import AutoTokenizer

# Load the ProtBert tokenizer
tokenizer = AutoTokenizer.from_pretrained("Rostlab/prot_bert_bfd", do_lower_case=False)

def tokenize_sequences(sequences, labels, max_len=190): #max_length is set to 190 based on the visualization of the AMPs and non-AMPs
    # Tokenize with padding and truncation
    encodings = tokenizer(
        sequences,
        padding='max_length',
        truncation=True,
        max_length=max_len,
        return_tensors='tf'
    )

    # Convert labels to tensor
    labels = tf.convert_to_tensor(labels)

    # Package into TensorFlow dataset
    dataset = tf.data.Dataset.from_tensor_slices((
        dict(encodings),
        labels
    ))

    return dataset

train_ds = tokenize_sequences(X_train, y_train)
val_ds = tokenize_sequences(X_temp, y_temp)

test_ds = tokenize_sequences(X_test, y_test)
test_ds = test_ds.batch(8)

# Add batching and prefetching for performance
train_ds = train_ds.shuffle(1000).batch(8).prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.batch(8).prefetch(tf.data.AUTOTUNE)

from transformers import TFAutoModelForSequenceClassification

# Load model: binary classification (num_labels = 2)
model = TFAutoModelForSequenceClassification.from_pretrained(
    "Rostlab/prot_bert_bfd",
    num_labels=2
)

# optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

model.compile(
    optimizer='adam',
    loss=loss,
    metrics=['accuracy']
)

tf.keras.backend.set_value(model.optimizer.learning_rate, 2e-5)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=8,  # You can reduce or increase depending on results
)

import matplotlib.pyplot as plt

# Plot accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


# Plot loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# import numpy as np
from sklearn.metrics import classification_report

# 1. Get predictions from the model on the test dataset
raw_predictions = model.predict(test_ds)

# 2. Convert the raw predictions (logits) to class labels
#    The argmax function finds the index of the highest value
#    for each prediction, which corresponds to the predicted class (0 or 1).
predicted_labels = np.argmax(raw_predictions.logits, axis=1)

# 3. Get the true labels from the test dataset
#    We need to extract the labels from the tf.data.Dataset object
true_labels = np.concatenate([y.numpy() for x, y in test_ds], axis=0)

# 4. Generate the classification report
#    Now that both `true_labels` and `predicted_labels` are in the correct format,
#    the function will work.
print(classification_report(true_labels, predicted_labels))

# import numpy as np
import seaborn as sns
# import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Generate the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plotting the confusion matrix as a heatmap for better visualization
plt.figure(figsize=(7, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['non-AMP', 'AMP'], yticklabels=['non-AMP', 'AMP'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

print("\nConfusion Matrix:")
print(cm)

from Bio import SeqIO
import random

# Set a seed for reproducibility. Use any integer you like.
random_seed = 42
random.seed(random_seed)

# File paths
input_fasta = "/content/drive/MyDrive/Pharm.D_Project/data/family_filtered.orfs.faa"
output_fasta = "/content/drive/MyDrive/Pharm.D_Project/data/sampled_orfs_subset5k.fasta"

# Parameters
k = 5000  # Number of sequences to sample
reservoir = []

# Step 1: Stream and apply reservoir sampling
for i, record in enumerate(SeqIO.parse(input_fasta, "fasta")):
    if i < k:
        reservoir.append(record)
    else:
        j = random.randint(0, i)
        if j < k:
            reservoir[j] = record

# Step 2: Write sampled sequences to new FASTA file
with open(output_fasta, "w") as f_out:
    SeqIO.write(reservoir, f_out, "fasta")

print(f"✅ Sampled 5,000 sequences and saved to: {output_fasta}")
print(f"The sampling is reproducible with seed: {random_seed}")

# Load the FASTA file containing all ORFs
# NOTE: This is the same path as your orf_path variable from before.
orf_path = "/content/drive/MyDrive/Pharm.D_Project/data/sampled_orfs_subset5k.fasta"
all_orf_records = list(SeqIO.parse(orf_path, "fasta"))

# --- Preprocessing ---
# You must apply the exact same preprocessing steps as you did for your training data.
def space_sequence(seq):
    return ' '.join(str(seq).replace(" ", ""))

# Apply spacing to all ORFs
spaced_orfs = [space_sequence(record.seq) for record in all_orf_records]

# Store the original records for later retrieval
original_orfs = [record for record in all_orf_records]
filtered_orfs = []
filtered_sequences = []

# Filter based on length (at least 8 amino acids)
# This ensures consistency with your training data.
for record, seq in zip(original_orfs, spaced_orfs):
    if len(seq.replace(" ", "")) >= 9:
        filtered_orfs.append(record)
        filtered_sequences.append(seq)

print(f"Total ORFs loaded: {len(all_orf_records)}")
print(f"ORFs after filtering: {len(filtered_orfs)}")


# --- Tokenization and Dataset Creation ---
# Reuse the tokenize_sequences function you defined earlier
# You don't need labels for this step, so we'll pass a dummy list of zeros
dummy_labels = [0] * len(filtered_sequences)
prediction_ds = tokenize_sequences(filtered_sequences, dummy_labels)
prediction_ds = prediction_ds.batch(8) # Use the same batch size as training

# --- Making Predictions ---
# Use your trained model to get raw predictions (logits)
print("\nMaking predictions on all ORFs...")
raw_predictions = model.predict(prediction_ds)

# --- Post-processing Predictions ---
# Convert the logits to class labels (0 or 1)
predicted_labels = np.argmax(raw_predictions.logits, axis=1)

# --- Filtering for New AMP Candidates ---
# Create a list to store the predicted AMP records
new_amp_candidates = []
for i, prediction in enumerate(predicted_labels):
    # Check if the model predicted class 1 (AMP)
    if prediction == 1:
        new_amp_candidates.append(filtered_orfs[i])

print(f"\nFound {len(new_amp_candidates)} new AMP candidates!")

# --- Saving the Results ---
# Save the new candidates to a FASTA file for future use
output_path = "/content/sample_data/Pharm.D_Project/new_AMP_candidates.fasta"
SeqIO.write(new_amp_candidates, output_path, "fasta")

print(f"Saved new AMP candidates to: {output_path}")

# Load the FASTA file containing all ORFs
orf_path = "/content/drive/MyDrive/Pharm.D_Project/data/sampled_orfs_subset5k.fasta"
all_orf_records = list(SeqIO.parse(orf_path, "fasta"))

# --- Preprocessing ---
# You must apply the exact same preprocessing steps as you did for your training data.
def space_sequence(seq):
    return ' '.join(str(seq).replace(" ", ""))

# Apply spacing to all ORFs
spaced_orfs = [space_sequence(record.seq) for record in all_orf_records]

# Filter based on length and store both original record and spaced sequence
filtered_orf_records = []
filtered_sequences = []

for record, seq in zip(all_orf_records, spaced_orfs):
    if len(seq.replace(" ", "")) >= 9:
        filtered_orf_records.append(record)
        filtered_sequences.append(seq)

print(f"Total ORFs loaded: {len(all_orf_records)}")
print(f"ORFs after filtering: {len(filtered_orf_records)}")


# --- Tokenization and Dataset Creation ---
# Reuse the tokenize_sequences function you defined earlier
dummy_labels = [0] * len(filtered_sequences)
prediction_ds = tokenize_sequences(filtered_sequences, dummy_labels)
prediction_ds = prediction_ds.batch(8)

# --- Making Predictions and Getting Probabilities ---
print("\nMaking predictions on all ORFs...")
raw_predictions = model.predict(prediction_ds)

# The model outputs logits, we need to convert them to probabilities
probabilities = tf.nn.softmax(raw_predictions.logits, axis=-1).numpy()
# Get the predicted class (0 or 1)
predicted_labels = np.argmax(probabilities, axis=1)

# --- Creating and Saving the Excel File ---
print("Generating Excel report...")

# Prepare data for the DataFrame
data = {
    'ORF_ID': [record.id for record in filtered_orf_records],
    'Sequence': [str(record.seq) for record in filtered_orf_records],
    'Predicted_Class': predicted_labels,
    'Prob_non_AMP': probabilities[:, 0], # Probability for class 0
    'Prob_AMP': probabilities[:, 1]       # Probability for class 1
}

# Create a pandas DataFrame
predictions_df = pd.DataFrame(data)

# Sort by the probability of being an AMP in descending order
predictions_df = predictions_df.sort_values(by='Prob_AMP', ascending=False)

# Save the DataFrame to an Excel file
output_excel_path = "/content/sample_data/Pharm.D_Project/new_AMP_predictions.xlsx"
predictions_df.to_excel(output_excel_path, index=False)

print(f"\nPrediction report saved to: {output_excel_path}")